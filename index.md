---
layout: default
---

# Workshop Description

## Course Description

Mathematical optimization serves as the foundation for various applications in science and engineering by providing formal tools to determine the optimal action, design, control, or model from a range of possibilities. In the fields of data science, machine learning, and artificial intelligence, mathematical optimization plays a crucial role in model training and learning.

This workshop will offer an overview of the fundamental aspects of mathematical optimization, including unconstrained and constrained optimization, convex optimization, and optimization for model training. With a practical focus, participants will actively engage in formulating and solving optimization problems using standard modeling languages and solvers throughout the workshop. By incorporating common models from machine learning and other disciplines, the aim is to familiarize participants with optimization tools, enabling them to apply them in their own work for rapid prototyping and experimentation.

The workshop will cover various topics, including formulating optimization problems, the fundamentals of constrained and unconstrained optimization, convex optimization, optimization methods for model fitting in machine learning, and optimization in Python using libraries such as SciPy and CVXPY. Throughout, in-depth Jupyter Notebook examples from machine learning, statistics, and other fields will be presented to illustrate the practical applications of optimization techniques.

## Prerequisites

Want to make sure this is the right workshop for you? Check out the workshop’s prerequisites below.

To benefit from this workshop, participants should have a comfortable understanding of linear algebra, differential multivariable calculus, and basic probability and statistics. Although experience with Python will be helpful, it is not mandatory.
- Proficiency in Linear Algebra, Differential Multivariable Calculus, and
  Basic Probability and Statistics: Participants should have a solid
  understanding of linear algebra, including matrix operations, vector spaces,
  and eigenvectors. Knowledge of differential multivariable calculus, such as
  partial derivatives and gradients, is important for understanding
  optimization algorithms. Additionally, a basic understanding of probability
  and statistics will be beneficial for interpreting optimization results and
  evaluating models.
- Familiarity with Modeling and Problem Formulation: Participants should have
  some familiarity with the process of formulating problems as mathematical
  models. This includes understanding how to translate real-world problems
  into mathematical expressions, formulate objective functions, and define
  constraints. Prior exposure to mathematical modeling in fields such as
  engineering, physics, or economics will be advantageous.
- *Optional*: Experience with Python (helpful but not required): While
  experience with Python is not mandatory, it will be helpful for participants
  to have some familiarity with the language. Python is commonly used for
  optimization and machine learning tasks, and participants with prior
  experience in Python will find it easier to implement and experiment with
  optimization techniques using libraries such as SciPy and CVXPY. However,
  the workshop will provide guidance and examples in Python for those who are
  new to the language.

To join the workshop, participants will need a device with a recent web
browser and two-way audio and video access to Zoom. This can be a laptop or
desktop computer running any operating system (Windows, Mac, or Linux).
Participative activities may benefit from a larger screen, so joining via a
smartphone or tablet is not recommended.


# About the Instructors

![Kevin Carlberg](/assets/img/profile.jpg){:style="max-width:30%;"}

[Kevin Carlberg](https://kevintcarlberg.net) is a Director of AI Research Science at Meta
and an Affiliate Associate Professor of Applied Mathematics and
Mechanical Engineering at the University of Washington. He leads a research
team focused on enabling the future of augmented and virtual reality through
AI-driven innovations. His individual research combines concepts from machine
learning, computational physics, and high-performance computing to drastically
reduce the cost of simulating nonlinear dynamical systems at extreme scale.
Previously, Kevin was a Distinguished Member of Technical Staff at Sandia
National Laboratories in Livermore, California, where he led a research group
of PhD students, postdocs, and technical staff in applying these techniques to
a range of national-security applications in mechanical and aerospace
engineering.

![Salil Deshpande](/assets/img/profile_salil.png){:style="max-width:30%;"}

Salil Deshpande is a PhD student at Stanford ICME. He does research
in the field of computational genomics, developing deep learning and
machine learning techniques to investigate the function of the
noncoding genome. Previously, he studied Chemical Engineering and 
Mathematics at UT Austin, where he did research in quantum chemistry and 
materials physics.

# Workshop Materials

## Pre-workshop Checklist

**Step 1: Join the class Piazza**
We will be using Piazza for questions and answers. Piazza will allow you to ask questions to the instructors before or during the class. Here is how to sign up:
- Go to the [class signup page](https://piazza.com/stanford/summer2023/cmesw09). You should see a website that looks like the image below:
![Piazza Signup Page](/assets/img/piazza_signup_page.png)
- Join the class by entering in Class Access Code icme2023optimization.
- You may need to provide an email address if you do not already have a Piazza account
Once you've joined the class, you can submit questions by clicking the blue **New Post** button at the top left of the webpage.

**Step 2: Fill out the pre-course survey**
Please fill out this quick 2-minute [pre-course survey](https://forms.gle/F3BWp57NDzxxwYz19)! You will need so sign in with your email address to fill out this form.  Filling this out will help us tweak the course contents to meet your needs!

**Step 3: Access the course contents**
The course slides and tutorials are available via Google Drive. Here is how to access the course contents:
- Go to [https://drive.google.com](https://drive.google.com) and log in with your Google
  account
- Go to [https://tinyurl.com/icmemathopt2023](https://tinyurl.com/icmemathopt2023) to access
  the shared Google Drive folder
- Click on "ICME Summer Workshop 2023 - Introduction to Mathematical Optimization" > "Add Shortcut to Drive".

**Step 4: Try out the tutorials for yourself!**
The class tutorials are hosted on Google Collab. [Google Colab](https://colab.research.google.com/) is a free cloud-based Jupyter Notebook environment.
The Python Notebooks in this course’s shared Drive contain real-world examples of optimization problems that you can explore and study by running them using Google Colab. Here are the steps to get started:
- In Google Drive, go to "ICME Summer Workshop 2023 - Introduction To Mathematical
  Optimization" > "Notebooks"
- Double click any notebook
- You now have three options:
  1. *Look at the notebook and its output*: Simply scroll around.
  2. *Interact with the notebook*: Feel free to modify the notebook and run it
     with modifications. However, because this remains part of the original folder, you
     do not have edit permissions and your changes won't be saved after
       exiting.
  3. *Make a copy of the notebook*: Click on “Copy to Drive” at the top, which will create a copy in "My Drive" > "Colab Notebooks". **This is your own separate copy.**

## Schedule

#### Session 1 (Wednesday, August 2, 8:00 - 9:30 A.M. PDT)
  - Introduction to optimization
  - Unconstrained optimization

#### Session 2 (Wednesday, August 5, 8:45 - 11:00 A.M. PDT)
  - Optimization in Python

#### Session 3 (Thursday, August 8, 8:00 - 9:15 A.M. PDT)
  - Constrained optimization
  - Optimization for machine learning

#### Session 4 (Thursday, August 8, 9:30 - 11:00 A.M. PDT)
  - Convex optimization
  - Convex optimization examples
  - Closing Q&A

## Additional Resources

Here are some additional resources for various topics:

- J. Nocedal and S. J. Wright. *Numerical Optimization*, Springer, 1999.
- S. Boyd and L. Vadenberghe. *Convex Optimization*, Cambridge University
  Press, 2004. [(available online)](http://stanford.edu/~boyd/cvxbook/)
  - Excellent lectures by S. Boyd online
  - Class notes and lectures for
    [EE364a](http://web.stanford.edu/class/ee364a/),
    [EE364b](http://web.stanford.edu/class/ee364b/) online
	- [CVX101 MOOC](https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about)
- P.E. Gill, W. Murray, and M.H. Wright, *Practical Optimization*, London,
  Academic Press, 1981.
- Bottou, L., Curtis, F.E. and Nocedal, J., 2018. Optimization methods for
  large-scale machine learning. SIAM Review, 60(2), pp.223-311. [(available on
  the arXiv)](https://arxiv.org/abs/1606.04838) (Advanced review article)
